---
title: "Exercises of Chapter 6: Non-uniformly Distributed Random Number Generation"
author: "Qihun Yang"
date: "2023-03-12"
output: html_document
---

## Ex 1
1.
```{r 100}
set.seed(1)
n = 100
r <- runif(n, min = 0, max = 1)
for (i in 1:n)
{
  if (r[i]>1/3)
    r[i] = 2
  else
    r[i] = 1
}
sum(r == 1)/sum(r!=0)
z=sum(r == 1)/sum(r!=0)
z=(z-(1/3)*n)/sqrt(n*(1/3)*(2/3))
2*pnorm(z)

```
2.
```{r 1000}
set.seed(1)
n = 1000
r <- runif(n, min = 0, max = 1)
for (i in 1:n)
{
  if (r[i]>1/3)
    r[i] = 2
  else
    r[i] = 1
}
sum(r == 1)/sum(r!=0)
z=sum(r == 1)/sum(r!=0)
z=(z-(1/3)*n)/sqrt(n*(1/3)*(2/3))
2*pnorm(z)
```
3.
```{r 10000}
set.seed(1)
n = 10000
r <- runif(n, min = 0, max = 1)
for (i in 1:n)
{
  if (r[i]>1/3)
    r[i] = 2
  else
    r[i] = 1
}
sum(r == 1)/sum(r!=0)
z=sum(r == 1)/sum(r!=0)
z=(z-(1/3)*n)/sqrt(n*(1/3)*(2/3))
2*pnorm(z)
```
4.
For the condition $P(X=1)=1/3$，we can conduct a hypothesis test:
$$H_0:P(X=1)=1/3,\ H_1:P(X=1)\neq1/3$$
Under the null hypothesis，the random numbers follow the distribution: $Bin(n,1/3)$. According to the central limit theorem, binomial distribution is equivalent to normal distribution if $n\to \infty$. Based on this asymptotic distribution，we can obtain the p-value of the three situations (see the results of three questions above). All p-values are close to 0, and thus we do not reject the null hypothesis.

## Ex 3
```{r cards}
m <- 54
n <- 10000
T <- c()
x0 <- 1:m

for (i in 1:n)
{
  set.seed(i)
  x <- sample(x0) # Shuffle the cards
  T[i] = sum(x-x0==0)
}
mean(T)
var(T)
```
Now derive the disstribu tion of $T$: 

$$P\{T=k\}=\frac{{54 \choose k}(54-k)!\sum_{i=0}^{54-k}\frac{(-1)^i}{i!}}{54!}$$

## Ex 4
```{r dice}
n <- 1000
T <- c()
start <- 2
end <- 12


for (i in 1:n) {
  index <- data.frame(start:end,c(rep(TRUE,end-start+1)))
  num <- 0
  set.seed(i)
  while (as.logical(sum(index[,2])) == TRUE)
  {
    x1 <- ceiling(6*runif(1, min = 0, max = 1))
    x2 <- ceiling(6*runif(1, min = 0, max = 1))
    temp <- x1+x2
    if (index[index[,1]==temp,2])
      index[index[,1]==temp,2] = FALSE
    num = num +1
  }
  T[i] = num
}
mean(T)
var(T)
```

## Ex 9

Directly sample from $\exp(1)$ distribution and keep those less than $0.05$ until obtaining $1000$ random numbers.
```{r 15.1}
set.seed(1)
n = 1000
count = 0
X <- c()
while (count < n){
  r <- -log(runif(1,0,1))
  if (r < 0.05){
    count = count+1
    X[count] <- r 
  }
}
```
Expectation:
```{r 15.2}
mean(X)
```
The following theoretical derivation:
\begin{aligned}
E(X|X<0.05)&=\int_{0}^{0.05}\frac{xe^{-x}}{1-e^{-0.05}}dx \\
&=\frac{-1}{1-e^{-0.05}}\int_{0}^{0.05}xde^{-x} \\
&=\frac{-1}{1-e^{-0.05}}(xe^{-x}|_0^{0.05}-\int_{0}^{0.05}e^{-x})dx \\
&=\frac{-1}{1-e^{-0.05}}(0.05e^{-0.05}+e^{-0.05}-1)
\end{aligned}
theoretical variance:
```{r 15.3}
(0.05*exp(-0.05)+exp(-0.05)-1)/(exp(-0.05)-1)
```
Two expectations are approximately equal.

## Ex 12

I adopt the rejection sampling in this problem, where the key process lies in the selection of envelope $g(x)$. I evaluate the efficiency of the method by examine the execution time.

( 1 )

$g(x)=\frac{1}{2}e^{-\frac{1}{2}x},\ x>0$

$$h(x)=\frac{p(x)}{g(x)}=2xe^{-\frac{1}{2}x}$$
$$h^{\prime}(x)=(2-x)e^{-\frac{1}{2}x}$$
To maximize $h(x)$, let $c=\max{h(x)}=h(2)=4/e$. 
The final expression of $h(x)$ is:
$$h(x)=\frac{p(x)}{g(x)}=\frac{x}{2}e^{-\frac{1}{2}x+1}$$

Therefore, we can obtain the random number according to the following algorithm.
```{r 20.1}
set.seed(1)
n = 1000
count = 0
X <- c()
s=Sys.time()
while (count < n){
  while (TRUE) {
    r <- -0.5*log(runif(1,0,1))
    Y <- runif(1,0,1)
    if (Y > (r/2)*exp(-0.5*r+1)){
      next
    }
    else{
      count = count+1
      X[count] <- r
      break
    }
  }
}
e=Sys.time()
#operation time
print(e-s)
```

( 2 )

$g(x)=\frac{1}{8}e^{-\frac{1}{8}x},\ x>0$

$$h(x)=\frac{p(x)}{g(x)}=8xe^{-\frac{7}{8}x}$$
$$h^{\prime}(x)=(8-7x)e^{-\frac{1}{2}x}$$
To maximize $h(x)$, let $c=\max{h(x)}=h(8/7)=64/7e$
The final expression of $h(x)$ is:
$$h(x)=\frac{p(x)}{g(x)}=\frac{7x}{8}e^{-\frac{7}{8}x+1}$$
```{r 20.2}
set.seed(1)
n = 1000
count = 0
X <- c()
s=Sys.time()
while (count < n){
  while (TRUE) {
    r <- -0.5*log(runif(1,0,1))
    Y <- runif(1,0,1)
    if (Y > (7*r/8)*exp(-0.875*r+1)){
      next
    }
    else{
      count = count+1
      X[count] <- r
      break
    }
  }
}
e=Sys.time()
#operation time
print(e-s)
```

It can be seen that the second algorithm takes about half the time of the first algorithm, so the second algorithm is more efficient. 

## Ex 15


( 1 ) Inverse cdf method

the Cumulative Distribution Function is 
$$
F(x)=
\left\{
\begin{array}{lr}\frac{1}{2}e^x,& x<0\\
1-\frac{1}{2}e^{-x},& x\geq 0
\end{array}
\right.
$$
Inverse Function:
$$
F^{-1}(y)=
\left\{
\begin{array}{lr}\ln2y,& x<0\\
-\ln(2-2y),& x\geq 0
\end{array}
\right.
$$
Algorithm:
```{r 23.1}
r <- runif(1,0,1)
if (r < 0.5){
   X <- log(2*r)
} else {
  X <- -log(2-2*r)
}
X
```

( 2 ) Mixture Sampling

The pdf can be represented as:
$$
p(x)=\frac{1}{2}e^xI\{x<0\}+\frac{1}{2}e^{-x}I\{x\geq0\}
$$
that is, $P\{I=1\}=P\{I=2\}=\frac{1}{2}$,
from which we obtain the algorithm:
```{r 23.2}
u1 <- runif(1,0,1)
if (u1 < 0.5){
  u2 <- runif(1,0,1)
  X <- log(u2)
} else {
  u2 <- runif(1,0,1)
  X <- -log(u2)
}
X
```

